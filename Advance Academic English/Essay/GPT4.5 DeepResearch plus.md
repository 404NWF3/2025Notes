# Background: The Rise of Generative AI and Academic Integrity Concerns (2018–Present)  
Generative artificial intelligence (AI) – AI that can produce human-like text, code, images, and other content – has advanced rapidly since 2018, raising new challenges for academic integrity. Early large language models (LLMs) like OpenAI’s **GPT-2** (2019) demonstrated the ability to generate coherent paragraphs of text from prompts, blurring the line between human and machine writing. OpenAI initially deemed GPT-2 “too dangerous” to fully release due to concerns it could be misused for generating fake content ([This news article about the full public release of OpenAI's 'dangerous ...](https://www.theregister.com/2019/11/06/openai_gpt2_released/#:~:text=,articles%2C%20phishing%20and%20spam)). By 2020, **GPT-3**, a much larger model (175 billion parameters), could produce convincing essays and answers, even fooling readers – for example, a *Guardian* article authored by GPT-3 went viral as many found it indistinguishable from human writing ([](https://prism.ucalgary.ca/server/api/core/bitstreams/add8f9c6-647a-44f7-b9be-be237874e496/content#:~:text=general%20public%20Fall%202020%3A%20Limited,on%20Reddit%20and%20Hacker%20News)). These models signaled that AI could potentially be used by students to write assignments or papers, bypassing traditional plagiarism checks. Unlike copy-paste plagiarism, AI-generated text is original (not directly copied from existing sources), allowing it to **evade standard plagiarism detectors** that compare text against databases ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=It%20is%2C%20without%20question%2C%20too,existing%20work%20and)). This raised alarms that students might use AI as an untraceable “ghostwriter” for essays, exam solutions, or even theses, challenging core principles of academic honesty.

Even before generative AI became mainstream, academic integrity frameworks struggled with issues like contract cheating and essay mills. Now AI tools offer a new form of automated, on-demand assistance that can produce content in seconds. By late 2022, OpenAI’s release of **ChatGPT** (based on GPT-3.5) brought generative AI to the masses, amassing over 100 million users within two months ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=Fast,%E2%80%9D)). ChatGPT’s accessibility and fluent output have made it “tailor-made to craft the kinds of essays that instructors ask for,” as one educator noted ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=can%20easily%20ask%20ChatGPT%20to,essays%20that%20instructors%20ask%20for)). In early 2023, headlines proclaimed “the college essay is dead,” dubbing the tool “**CheatGPT**” in popular media ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=Once%20the%20tool%E2%80%99s%20capabilities%20became,of%20academic%20writing%20is%20lost)). While such doomsday scenarios are debated, there is broad agreement that generative AI poses **legitimate concerns for academic integrity** ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=One%20need%20not%20accept%20this,to%20address%20this%20new%20reality)). Institutions worldwide have been forced into “emergency mode” to update honor codes, detection methods, and teaching strategies in response ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=%E2%80%9CIt%27s%20been%20a%20very%20loud,at%20all%20levels%2C%E2%80%9D%20he%20said)) ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=On%20Monday%2C%20the%20honor%20council,%E2%80%9D)).

In parallel, **policy makers and educational organizations** have begun grappling with how to govern AI usage in education. International bodies like UNESCO recognized early on the importance of AI in education – the 2019 *Beijing Consensus on AI and Education*, for instance, urged that AI be developed to support educational goals ethically (a prescient concern just as GPT-2 emerged). By 2021, UNESCO’s global **Recommendation on the Ethics of AI** was adopted, establishing principles (like transparency and accountability) that also inform academic settings. These efforts laid a groundwork for **AI governance**, but the lightning-fast adoption of tools like ChatGPT in late 2022 dramatically accelerated the timeline for practical policy responses in academia.

This review provides a historical overview of academic integrity challenges related to generative AI from 2018 to the present. It examines major AI technological advancements that have impacted academic integrity (such as GPT-2, GPT-3, GPT-4 and other generative models), surveys significant academic policies and institutional responses to AI in education, and offers a comparative analysis of approaches in China versus Western countries. Key regulatory milestones, influential studies, and landmark events are highlighted to illustrate how AI governance in academic settings has evolved. Finally, we discuss future implications, including how educational institutions might adapt to ensure integrity in an AI-pervasive future.

# Key Developments and Challenges in the Generative AI Era  

## Breakthroughs in Generative AI (2018–2023) and Their Impact  
- **2018–2019 – Early Generative Models:** OpenAI’s first Generative Pre-trained Transformer, **GPT-1 (2018)**, was a research milestone but little known outside AI labs. The watershed came with **GPT-2 in February 2019**, which was trained on 40 GB of internet text and could produce surprisingly coherent passages ([Fake news model in staged release but two researchers fire up replication](https://techxplore.com/news/2019-08-fake-news-staged-replication.pdf#:~:text=replication%20techxplore,2)). Fearing misuse for “deceptive or abusive” purposes, OpenAI initially released only a scaled-down GPT-2 model ([This news article about the full public release of OpenAI's 'dangerous ...](https://www.theregister.com/2019/11/06/openai_gpt2_released/#:~:text=,articles%2C%20phishing%20and%20spam)). By late 2019, GPT-2 was fully released to the public ([](https://prism.ucalgary.ca/server/api/core/bitstreams/add8f9c6-647a-44f7-b9be-be237874e496/content#:~:text=November%202019%3A%20GPT,built%20functions.%20Free)), giving researchers and tech-savvy students a first taste of AI-generated text. Although GPT-2 often wandered off-topic, it proved that AI could generate fluent writing – enough to spark early concerns about AI-driven plagiarism. Academic integrity discussions at this stage were speculative, as GPT-2’s availability was limited and its outputs still somewhat hit-or-miss.

- **2020 – GPT-3 and the “AI Author”:** The game-changer was **GPT-3**, unveiled in mid-2020. With 100× more parameters than GPT-2, GPT-3 could generate lengthy, articulate responses and even answer complex questions. In September 2020, *The Guardian* published an op-ed titled “A robot wrote this entire article. Are you scared yet, human?”, which was written by GPT-3 with minimal editing ([](https://prism.ucalgary.ca/server/api/core/bitstreams/add8f9c6-647a-44f7-b9be-be237874e496/content#:~:text=general%20public%20Fall%202020%3A%20Limited,on%20Reddit%20and%20Hacker%20News)). Examples of GPT-3 text **passing as human-written** appeared on Reddit and Hacker News later that year ([](https://prism.ucalgary.ca/server/api/core/bitstreams/add8f9c6-647a-44f7-b9be-be237874e496/content#:~:text=general%20public%20Fall%202020%3A%20Limited,on%20Reddit%20and%20Hacker%20News)). Such cases demonstrated that AI-generated essays could potentially fool readers and instructors. By late 2020, OpenAI made GPT-3 available through an API, and **Microsoft began integrating GPT-3’s autocomplete capabilities into Word’s Editor** feature ([](https://prism.ucalgary.ca/server/api/core/bitstreams/add8f9c6-647a-44f7-b9be-be237874e496/content#:~:text=Hacker%20News.%20Winter%202020%3A%20GPT,2%20released%20to%20public%20through)) – signaling that AI text generation was entering everyday tools. While still not broadly accessible to all students, GPT-3 showed that **entire assignments could be authored by AI** with minimal human input. Early adopters in academia noted that GPT-3 could answer short-answer questions or write passable essays on generic topics, raising flags that the traditional take-home essay might need rethinking.

- **2021–2022 – Proliferation of Generative AI:** Following GPT-3, the AI field saw a proliferation of generative models. Open-source efforts (EleutherAI’s **GPT-Neo** and **GPT-J**, Google’s **FLAN** models, etc.) put advanced LLMs into more hands. Specialized models also emerged: OpenAI’s **Codex** (2021) could generate computer code, later powering GitHub **Copilot** – an AI coding assistant. This introduced **academic integrity concerns in computer science education**: instructors observed that Copilot could instantly solve programming assignments, even unusual ones, because it was trained on vast repositories of code. A UMass Amherst professor warned in 2022 that *“students armed with Copilot will be bringing Uzis to a knife fight,”* since the tool can auto-complete many intro programming tasks correctly ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=%3E%20Microsoft%27s%20AI%20code,of%20it%20%E2%80%93%20pointless%20because)) ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=%3E%20,computer%20science%20classes%2C%20and%20especially)). Educators realized they might need to redesign coding projects or teach with AI rather than against it ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=,)) ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=,to%20sort%20a%20list%20of)). By 2022, text-generating AI became more user-friendly: **InstructGPT** (early 2022) improved the alignment of GPT-3 with user intents, and other tech companies launched chat-style assistants. However, the **landmark event was November 30, 2022**, when OpenAI released **ChatGPT** (based on the refined GPT-3.5). ChatGPT’s public web interface and free access removed any remaining barrier – millions of students and educators began experimenting with it overnight. Within weeks, ChatGPT had **over 100 million users** (the fastest-growing consumer application in history) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=Fast,%E2%80%9D)). This widespread adoption meant generative AI was no longer hypothetical in education; it was already being used to cheat on exams, write essays, or assist with homework on a massive scale ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=A%20poll%20of%20Stanford%20University,and%20outside%20of%29%20academia)).

- **2023 – Multi-Modal and Advanced AI (GPT-4 and beyond):** In March 2023, OpenAI launched **GPT-4**, pushing capabilities even further. GPT-4 demonstrated human-level performance on many academic and professional exams – scoring in the ~90th percentile on the bar exam and excelling in medical, AP, and graduate admission tests ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=version%2C%20GPT,%E2%80%9D)). Unlike earlier models, GPT-4 can handle **visual inputs** (images/diagrams) in addition to text, meaning it could potentially solve math problems from a diagram or analyze lab result charts. For academic integrity, GPT-4 showed that **almost no standard assessment was off-limits to AI** – from multiple-choice and essay exams to problem sets – unless the assessment design changed. By mid-2023, other competitors entered classrooms: **Google’s Bard** and **Anthropic’s Claude** (both large language model chatbots) became available, and **open-source LLMs** (like LLaMA, Alpaca, etc.) made it feasible for students to run capable AI on a personal computer. The rapid evolution of model quality has made it increasingly difficult to distinguish AI-written work from student-written work on content alone. Studies have confirmed that AI-generated text can slip past plagiarism detectors, which rely on matching text to known sources ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=It%20is%2C%20without%20question%2C%20too,existing%20work%20and)). Furthermore, while new **AI-text detection tools** have been developed (more on these below), sophisticated models are harder to reliably detect, especially as they improve and incorporate strategies to avoid obvious signatures. The generative AI boom of 2018–2023 thus fundamentally changed the landscape of academic integrity: today’s AI can produce original, cogent responses on almost any topic, challenging educators to adapt **how they assess learning** and **how they define permissible assistance**.

## Academic Integrity Challenges and Institutional Responses  

**Widespread Student Use:** By 2023, surveys confirmed that students were embracing generative AI at striking rates. A Stanford University poll found 17% of students admitted to using generative AI on fall 2022 coursework ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=A%20poll%20of%20Stanford%20University,and%20outside%20of%29%20academia)). In a broader survey of U.S. college students (early 2023), **43% reported using generative AI** tools for assignments ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=complete%20homework,and%20outside)). Another poll by Study.com found an even higher self-reported usage (over 89% of students had used ChatGPT for homework) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=A%20poll%20of%20Stanford%20University,and%20outside)), though that sample may have been skewed toward AI-aware respondents. Notably, many students do not see this as cheating – in one survey, **20% of students said using AI isn’t cheating or plagiarism**, and nearly half believed *“it is possible to use AI in an ethical way to help complete assignments”* ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=But%20not%20all%20students%20and,who%20disagreed)) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=do%20not%20view%20the%20use,who%20disagreed)). This generational perspective puts pressure on institutions to clarify their standards. By late 2023, generative AI had also permeated Chinese campuses: a China Youth Daily survey found **over 80% of Chinese college students used AI tools to assist with coursework** ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=essay%20content%2C)). In short, student adoption of AI has been *rapid and global*, forcing educators and administrators to react in real-time.

**Initial Alarm and Short-Term Bans:** The first response at many institutions was a defensive one – treating AI-generated work as a form of plagiarism or unauthorized aid under existing academic integrity codes. In January 2023, just weeks after ChatGPT’s release, several universities and school systems moved to **ban or restrict AI use**. For example, **Sciences Po (Paris)** issued a school-wide ban on ChatGPT (and any AI tool) for written work *“without transparent referencing,”* threatening sanctions up to expulsion for violators ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=In%20an%20email%20addressed%20to,higher%20education%20as%20a%20whole)). The policy made clear that students are *“forbidden to use the software for the production of any written work or presentations, except for specific course purposes…with the supervision of a course leader.”* ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=In%20an%20email%20addressed%20to,higher%20education%20as%20a%20whole)). This hardline stance by a leading French university was among the first high-profile bans in higher education. Similarly, the University of Hong Kong (HKU) announced a temporary ban on student use of ChatGPT or AI-based tools for coursework in January 2023, pending development of longer-term policies ([University of Hong Kong issues interim ban on ChatGPT, AI-based ... - CGTN](https://news.cgtn.com/news/2023-02-19/University-of-Hong-Kong-issues-interim-ban-on-ChatGPT-AI-based-tools-1hxWzqgcMxy/index.html#:~:text=University%20of%20Hong%20Kong%20issues,on%20AI%20tools%2C%20said)). In the United States, some public K-12 school districts (e.g. New York City schools) blocked ChatGPT on their networks and devices, reflecting fears that take-home assignments would be undermined. These **knee-jerk bans** were often compared to past reactions to new technology in education – effective as an immediate signal, but potentially unenforceable and even counter-productive if they drive AI use underground ([The Ethics of College Students Using ChatGPT - University Policy](https://ethicspolicy.unc.edu/news/2023/04/17/the-ethics-of-college-students-using-chatgpt/#:~:text=The%20Ethics%20of%20College%20Students,witchhunts%20for%20AI%20generated%20writing)) ([The Ethics of College Students Using ChatGPT - University Policy](https://universitypolicy.unc.edu/news/2023/04/17/the-ethics-of-college-students-using-chatgpt/#:~:text=Conversations%20between%20students%20and%20faculty,witchhunts%20for%20AI%20generated%20writing)). Critics warned against “witch hunts for AI-generated writing” ([The Ethics of College Students Using ChatGPT - University Policy](https://ethicspolicy.unc.edu/news/2023/04/17/the-ethics-of-college-students-using-chatgpt/#:~:text=The%20Ethics%20of%20College%20Students,witchhunts%20for%20AI%20generated%20writing)) and urged that outright prohibition without discussion could hinder learning about the technology itself.

**Evolution of Academic Policies (2023):** As the semester progressed, many universities shifted from outright bans to more nuanced policies and guidelines. A common approach was to update honor codes and academic integrity statements to explicitly address AI. For instance, **Stanford University’s Board on Conduct Affairs** issued guidance in Feb 2023: by default, using generative AI is treated **“analogously to assistance from another person”** – i.e. if an assignment forbids outside help, then using AI without permission violates the honor code ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Absent%20a%20clear%20statement%20from,such%20assistance%20when%20in%20doubt)). Stanford’s policy requires students to **acknowledge any use of AI** (when allowed) and makes clear that submitting AI-generated work as one’s own is unacceptable unless expressly permitted ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Absent%20a%20clear%20statement%20from,such%20assistance%20when%20in%20doubt)) ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Students%20should%20acknowledge%20the%20use,such%20assistance%20when%20in%20doubt)). At the same time, Stanford explicitly left room for instructors to **set their own AI policies** for their courses, including allowing it for certain tasks or integrating it as a learning tool ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=and%20default%20to%20disclosing%20such,assistance%20when%20in%20doubt)) ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Individual%20course%20instructors%20are%20free,ask%20their%20instructors%20for%20clarification)). This approach – *providing a default rule but empowering instructors to make exceptions* – became a template for many Western universities. Harvard, Princeton, and others issued similar guidance: unauthorized AI use = unauthorized aid (cheating), but instructors may assign work that includes AI or teach proper use, in which case it must be disclosed and cited. Some universities created resource pages for faculty and students on “AI and academic integrity” and offered sample syllabus statements (for example, the University of Pennsylvania’s Center for Teaching and Learning published sample statements for different levels of AI allowance) ([Academic Integrity Statements that Address Generative AI](https://cetli.upenn.edu/resources/syllabus/academic-integrity-statements-that-address-generative-ai/#:~:text=Academic%20Integrity%20Statements%20that%20Address,using%20these%20tools%20to%20generate)) ([Academic Integrity and Teaching With(out) AI – Office of Academic ...](https://oaisc.fas.harvard.edu/academic-integrity-and-teaching-without-ai/#:~:text=Academic%20Integrity%20and%20Teaching%20With,list%20of%20resources%20for)).

**Adaptation in Assessment Design:** Educators began adjusting **assessment strategies** to mitigate easy AI cheating. A notable trend in early 2023 was a move away from unsupervised, take-home essays and toward more controlled assessments. Some faculty replaced open-book take-home exams with **in-class essays, oral exams, or handwritten work** to ensure students demonstrate knowledge without AI assistance ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=ethical%20way%20to%20help%20complete,who%20disagreed)) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=And%20some%20schools%20and%20professors,first%20draft%20in%20the%20classroom)). In Australia, several leading universities shifted back to in-person supervised exams and “paper-based” assessments for final evaluations, explicitly citing the challenge that ChatGPT poses to academic honesty in remote assessments ([CHATGPT AND ACADEMIC INTEGRITY: OPTIONS FOR ADAPTING ASSESSMENT IN ...](https://melbourne-cshe.unimelb.edu.au/__data/assets/pdf_file/0008/4533218/ChatGPT-and-Academic-Integrity.pdf#:~:text=CHATGPT%20AND%20ACADEMIC%20INTEGRITY%3A%20OPTIONS,vulnerable%20to%20plagiarism%20and%20cheating)). At the same time, forward-looking instructors experimented with *“constructive”* use of AI in assignments. For example, a professor at University of Pennsylvania’s Wharton School (Ethan Mollick) drew attention for **requiring his students to use ChatGPT** in certain assignments, treating AI proficiency as an emerging skill ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=guardrails%20are%20in%20place,first%20draft%20in%20the%20classroom)) ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=Ethan%20Mollick%2C%20an%20entrepreneurship%20and,to%20help%20with%20their%20classwork)). Mollick’s policy, announced in Jan 2023, had students use ChatGPT to brainstorm and draft ideas, but made them responsible for fact-checking and refining the output ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=,there%20for%20a%20long%20time)) ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=His%20new%20AI%20policy%20%E2%80%94,that%20the%20bot%20spits%20out)). He argues that *“there’s a lot of positives”* to AI and that **learning to work with AI will be crucial**, while of course *“cheating and negativity”* must be managed ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=Ethan%20Mollick%2C%20an%20entrepreneurship%20and,to%20help%20with%20their%20classwork)) ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=,there%20for%20a%20long%20time)). Other faculty adopted similar tactics: for instance, at Northern Michigan University, one professor had students write an initial essay draft in class and then use AI tools to revise and improve it as a second draft – provided they could explain and support the AI-assisted changes ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=computers%20or%20technology%2C%20others%20are,be%20sufficiently%20supported%20and%20explained)). These innovators report that when used under guidance, AI can enhance learning (e.g. helping non-native English speakers with grammar ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=He%20also%20said%20it%20can,as%20a%20result))) rather than replace it. Such pilots foreshadow how pedagogy might evolve to incorporate AI ethically.

**AI Detection Tools and “Arms Race”:** A major aspect of institutional response has been the search for reliable **AI-written text detectors**. Early on, computer science researchers and companies rolled out tools aiming to identify AI-generated text by subtle artifacts in wording or syntax. In January 2023, a Princeton student garnered media attention for creating “GPTZero,” a tool to flag AI-written essays. In tests, GPTZero could correctly identify a majority of AI samples, but it also yielded many false positives/negatives ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=scientist%2C%20so%20her%20impulse%20was,to%20run%20her%20own%20experiment)) ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=%E2%80%9CI%20did%20run%20nine%20submissions,%E2%80%9D)). OpenAI themselves released an official **AI text classifier** (Jan 31, 2023) but admitted it was not highly accurate (identifying only ~26% of AI text, with a 9% false positive rate). By April 2023, the popular plagiarism checker **Turnitin** integrated an AI writing detection feature, claiming 97% confidence in flagging GPT-generated text ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=It%20is%2C%20without%20question%2C%20too,existing%20work%20and)). However, educators quickly learned that no detector is foolproof – students can tweak AI output or use newer models to evade detection, and false accusations are a serious concern if a tool misidentifies a human student’s work as AI-generated. Some academic integrity officers cautioned faculty *not* to rely solely on detectors to catch cheating, but to use them as **one piece of evidence** alongside changes in a student’s writing style, suspicious citations (AI often produces fake references ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=There%20are%20other%20significant%20limitations,such%20tools%20are%20capable%20of))), or direct questioning of the student. This *“cat-and-mouse”* dynamic, where AI improves to avoid detection and detectors chase the AI, is ongoing. As of 2024, detection tools are widely used but generally kept as confidential aids by instructors (to prevent students from learning to game them). Notably, one detection approach that *has* been implemented in China is a quantitative analysis of content: some Chinese universities now scan theses for AI content and set a threshold (e.g. if >40% of a thesis is AI-generated, the student is warned or must rewrite) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Fuzhou%20University%20in%20Fujian%20province,be%20considered%20in%20grade%20evaluation)). We will discuss this further in the China section. Overall, detection offers partial help, but the consensus is that **long-term solutions must be pedagogical and policy-based** rather than purely technological ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=Issues%20for%20Academic%20Integrity)).

**Academic Research and Publications:** Generative AI has also impacted **research integrity and publishing**. In early 2023, there were instances of researchers listing ChatGPT as a co-author on papers, which led major science publishers (e.g. *Nature*, *Science*) to clarify that AI tools cannot be credited as an author since they cannot take responsibility for the work. Journals updated authorship and contribution guidelines to require disclosure if AI was used in writing or data analysis. A *landmark event* occurred in March 2024 when a paper by a Beijing-based professor was **retracted** from the journal *Surfaces and Interfaces* after it was discovered that the introduction contained text apparently generated by ChatGPT (including a common ChatGPT prompt phrase). The retraction notice cited *“concerns that the authors appear to have used a Generative AI source in the writing process without disclosure,”* calling this a breach of publication ethics ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=In%20March%2C%20a%20research%20paper,prompter%20in%20the%20article%27s%20introduction)). This was a clear signal that undisclosed AI assistance is viewed as academic misconduct in research as well as coursework. In response, universities (in China and the West alike) started instructing researchers and graduating students to **disclose any AI tool use** in theses and publications. For example, *Communications of the ACM* issued guidelines for authors on how to document AI usage. Conferences in AI itself took steps too – the International Conference on Machine Learning (ICML 2023) outright **banned submitted papers from containing any text generated by ChatGPT or similar**, unless that text was presented as part of the research experiment itself ([AI conference and NYC's educators ban papers done by ChatGPT](https://www.theregister.com/2023/01/06/ai_conference_nyc_ban/#:~:text=People%20are%20increasingly%20using%20them,the%20paper%27s%20experimental%20analysis)) ([AI conference and NYC's educators ban papers done by ChatGPT](https://www.theregister.com/2023/01/06/ai_conference_nyc_ban/#:~:text=,the%20paper%27s%20experimental%20analysis)). The ICML organizers noted the difficulty of detection but stated that authors found breaking this rule would be rejected for violating ethical norms ([AI conference and NYC's educators ban papers done by ChatGPT](https://www.theregister.com/2023/01/06/ai_conference_nyc_ban/#:~:text=AI%20conference%20and%20NYC%27s%20educators,the%20paper%27s%20experimental%20analysis)). These developments show the academic community striving to set norms around AI-generated content to preserve integrity of the scholarly record.

Taken together, 2018–2023 saw **escalating capabilities of AI and a spectrum of academic responses**. What began with speculative concern in 2019 became an urgent reality by 2023. Universities moved from panic (issuing bans) to more systematic adaptation: updating honor codes, devising new assignments, exploring detection, and even leveraging AI as a teaching tool. We now turn to a comparative look at how approaches have differed in China versus Western countries during this period, in terms of policies, regulations, and governance effectiveness.

# Comparative Analysis: China vs. Western Countries in AI Governance in Education  

Both China and Western countries have recognized the challenges generative AI poses to academic integrity, but their **policy approaches and governance frameworks** differ in emphasis and execution. Below we compare key aspects of how Chinese institutions and governments have responded versus their counterparts in the U.S. and other Western nations.

## Policy Approaches in Universities  
- **Western Universities:** In North America, Europe, and Australia, universities have largely handled AI policy at the **institution or faculty level**, often with flexibility for individual instructors. The typical Western approach has been to issue broad guidelines (as described, many analogize AI use to getting unauthorized help) and then let professors set specific rules in their syllabi ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=and%20default%20to%20disclosing%20such,assistance%20when%20in%20doubt)). Some institutions took a hard line (e.g. Sciences Po’s blanket ban in early 2023 ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=In%20an%20email%20addressed%20to,higher%20education%20as%20a%20whole))), but these were the exception. More common was a cautious integration: allow AI for certain learning activities, forbid it for graded work unless stated otherwise, and always require disclosure and proper citation of AI contributions. This decentralized policy approach means rules can vary widely even within a country or a single university system. For example, while one department might permit AI-assisted coding with attribution, another department might categorically prohibit any AI-written text on assignments. The **onus is often on students** to check each class’s policy, and on instructors to clearly communicate what AI use (if any) is acceptable ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Individual%20course%20instructors%20are%20free,ask%20their%20instructors%20for%20clarification)). The advantage of this approach is flexibility – policies can be tailored to the discipline (AI might be more acceptable in a computer science project than in a take-home literature essay). The downside is inconsistency and potential confusion; students need to navigate a patchwork of rules. Still, by mid-2023 a trend emerged in Western academia toward **updating honor codes** institution-wide to mention AI, while empowering constructive use. Western institutions also tend to frame AI **within existing academic integrity principles** – for instance, using AI without credit is treated similarly to plagiarism or cheating, thus enforceable under established disciplinary processes ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=Sciences%20Po%E2%80%99s%20senior%20management%20team,higher%20education%20as%20a%20whole)). This framing leverages the familiarity of concepts like plagiarism, even though AI-generation is a new mode.

- **Chinese Universities:** Chinese higher education institutions, often under guidance from the Ministry of Education, have taken a more **top-down and standardized approach** in crafting AI-related academic policies. Rather than leaving it purely to individual professors, many Chinese universities issued institute-wide regulations specifically addressing AI in academic work (especially for high-stakes work like theses). For example, **Fudan University (Shanghai)** released a comprehensive policy in late 2024 delineating exactly what is *permitted* versus *forbidden* when using AI for an undergraduate thesis ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20announced%20in%20its,writing%20process)) ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20document%20also%20specified%20conditions,existing%20ones%20and%20collate%20references)). Fudan’s policy bans use of generative AI tools in core research tasks – students may not use AI to decide thesis topics, design research methodologies, analyze data, or write any part of the main thesis body, results, or conclusions ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20announced%20in%20its,writing%20process)). AI can also not be used to **write or translate** even ancillary sections like acknowledgements ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=It%20also%20bans%20AI%20tools,language%20or%20do%20translation%20work)). However, Fudan *does* allow AI in specific supportive roles *with supervisor approval*: e.g. to help **retrieve literature, generate summaries or charts from existing data, or format references**, as long as this assistance does not replace the student’s own analysis or creative work ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20document%20also%20specified%20conditions,existing%20ones%20and%20collate%20references)). Crucially, students must **disclose exactly what tools they used and for what purpose in their thesis** ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20stipulation%20clearly%20stated%20that,theses%20were%20completed%20using%20them)). Other universities in China have implemented similar disclosure requirements. The Communication University of China in Beijing, for instance, now asks students to declare if they used generative AI in their thesis and to provide details like the model name and how the AI content was generated ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Communication%20University%20of%20China%20in,other%20authors%2C%20the%20notice%20said)) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=departments%20asking%20students%20to%20disclose,other%20authors%2C%20the%20notice%20said)). Chinese policies often explicitly state that if AI was used, the student is *ultimately responsible* for the content’s integrity and that AI cannot be credited as an author or creator ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=According%20to%20Fudan%20University%2C%20the,authors%20of%20theses)). These strict but clear rules aim to **preserve originality and academic integrity** while acknowledging AI as a tool ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=According%20to%20Fudan%20University%2C%20the,authors%20of%20theses)). Notably, Chinese universities have generally *not* completely banned AI assistance; instead, they forbid AI “ghostwriting” (i.e. having AI produce work that the student submits as their own) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Xiong%20Bingqi%2C%20director%20of%20the,of%20AI%20to%20ghostwrite%20papers)), but allow measured use for research efficiency. This reflects a recognition that AI can enhance productivity, but wanting to ensure the **student’s own intellectual contribution remains central** ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=More%20than%2080%20percent%20of,China%20Youth%20Daily%20in%20November)) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Reasonable%20and%20creative%20use%20of,of%20the%20student%27s%20independent%20research)). The consistency within each institution (and often similar rules across leading universities) provides clearer expectations to students. However, it requires robust enforcement mechanisms (such as thesis AI scans and advisor oversight) to be effective.

## Regulatory Frameworks and Governance Mechanisms  
- **China’s Regulatory Environment:** China’s approach is shaped by broader national regulations on AI. The Chinese government has been proactive in regulating generative AI at the macro level. A significant milestone was the introduction of the **Interim Measures for the Management of Generative AI Services** (effective August 15, 2023), which set nationwide rules for any generative AI service offered to the public ([China's New Rules For Generative AI: An Emerging Regulatory ... - Fasken](https://www.fasken.com/en/knowledge/2023/08/chinas-new-rules-for-generative-ai#:~:text=Fasken%20www,government%20agencies%2C%20including%20the)) ([China's New Rules For Generative AI: An Emerging Regulatory ... - Fasken](https://www.fasken.com/en/knowledge/2023/08/chinas-new-rules-for-generative-ai#:~:text=generative%20artificial%20intelligence%2C%20the%20Interim,government%20agencies%2C%20including%20the)). These measures require AI content to align with core socialist values and Chinese laws, mandate providers to implement content moderation and prevent misinformation, and crucially require labeling of AI-generated content in certain contexts. While the Interim Measures *exclude* generative AI developed and used internally by entities like schools or research institutions from their scope ([China's Interim Measures to Regulate Generative AI ... - China Briefing](https://www.china-briefing.com/news/how-to-interpret-chinas-first-effort-to-regulate-generative-ai-measures/#:~:text=Briefing%20www.china,public%20entities%20from%20their%20scope)), the overall regulatory stance influences academia. For example, because of these rules, **ChatGPT is not officially available in China** (it’s a foreign service not in compliance and is largely blocked online). Chinese students who use generative AI typically rely on either VPN access to foreign models or more likely on **domestic AI platforms** (like Baidu’s ERNIE Bot, Alibaba’s Tongyi Qianwen, iFlytek’s Spark, etc.), which are deployed in accordance with government regulations. These domestic models have built-in filters to prevent certain prohibited content and are monitored for misuse. From an academic integrity standpoint, China’s regulatory framework means universities operate in an environment where AI use can be more easily *monitored and controlled* at a systemic level. Additionally, in January 2023 China implemented regulations on “deep synthesis” technology (covering deepfakes and AI-generated media), requiring proper disclosure of AI-generated content and prohibiting use of AI to spread false information. Together, these regulations underscore that **transparency and accountability** are legally expected when using AI. Chinese universities have mirrored these concepts by requiring disclosure of AI usage in academic work ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Communication%20University%20of%20China%20in,other%20authors%2C%20the%20notice%20said)) and by developing internal **AI content detection systems** for theses ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=North%20China%20Electric%20Power%20University,interview%20with%20China%20Youth%20Daily)). In short, China’s governance mechanism is a blend of **top-down regulation and institutional enforcement**: government rules set the tone (ensuring AI does not undermine social or academic order), and universities implement that with concrete policies (like penalizing undisclosed AI ghostwriting with academic misconduct charges, up to degree revocation ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20said%20violations%20of,obtained%20degree%20will%20be%20annulled)) ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=deduction%20in%20graduation%20thesis%20scores,obtained%20degree%20will%20be%20annulled))).

- **Western Regulatory Landscape:** In contrast, Western countries by 2024 had no specific national laws aimed at AI use in education or academic cheating. The governance primarily comes from institutional policies and professional norms. However, broader AI regulations are under development: the **European Union’s AI Act** (expected to be finalized in 2024) will classify AI systems by risk. It’s likely that AI used in education (especially for evaluating students or generating educational content) will be considered “high risk,” requiring transparency and risk mitigation. The EU AI Act might, for example, mandate that if AI-generated content is used in student assessment, it must be disclosed or the AI system must meet certain standards. Additionally, data privacy laws (like GDPR in Europe) influence what student data can be fed into AI tools and could restrict cloud-based AI use depending on where data is processed. In the United States, there is not yet federal AI regulation, but there are ongoing discussions about AI in the Department of Education and among accreditation bodies. In lieu of laws, **academic organizations and consortia** have been issuing guidelines. For instance, the European University Association (EUA) in 2023 released recommendations for universities on adapting to AI, and various national teaching and learning centers have published white papers on “AI and academic integrity.” One recurring theme in Western governance is an emphasis on **ethical use and AI literacy** rather than prohibition. The narrative is that we should teach students *how* to use AI ethically (with attribution, critical verification of AI outputs, etc.) – this is reflected in many university policy statements that pair warnings about misconduct with encouragement of “ethical AI usage” skills ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=But%20not%20all%20students%20and,who%20disagreed)) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=do%20not%20view%20the%20use,who%20disagreed)). Furthermore, Western discussions often involve stakeholders like honor code councils, academic integrity offices, and student representatives to shape policy. For example, at Texas State University, the honor code council in early 2023 sent a letter to all faculty about AI implications, referencing the existing honor code language about originality and cautioning that submitting AI-written work as one’s own “**is an academic integrity violation**” akin to any other form of cheating ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=On%20Monday%2C%20the%20honor%20council,%E2%80%9D)) ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=%E2%80%9CAs%20we%20begin%20the%20second,%E2%80%9D)). This illustrates how Western governance leverages **existing honor code frameworks** to handle AI cases, rather than new government rules.

## Effectiveness and Cultural Perspectives  
- **Effectiveness in Curbing Misconduct:** It is still early to judge the long-term effectiveness of different approaches. Western institutions that banned AI outright faced immediate compliance issues – tech-savvy students could still access tools at home, and detecting usage was difficult unless a student made obvious mistakes (such as the now-notorious case of a student caught after ChatGPT produced *fictitious references*, tipping off the professor ([OpenAI ChatGPT and Biased Information in Higher Education](https://www.tamusa.edu/academics/ai-resources/documents/Open-AI-Chat-GPT-and-Bias-by-OBrien-and-Alsmadi.pdf#:~:text=GPT,schools%20ferret%20out%20AI%20plagiarism)) ([Is Using ChatGPT Plagiarism? An In-Depth Look At AI Writing And ...](https://expertbeacon.com/is-using-chatgpt-plagiarism-an-in-depth-look-at-ai-writing-and-academic-integrity/#:~:text=earlier%20version%20of%20the%20model,1))). The high percentages of students admitting AI use in surveys suggest that **informal use continued despite official disapproval**. On the other hand, Western institutions that embraced a balanced approach (teaching with AI, setting clear rules per assignment) may see less adversarial behavior and more student buy-in. Many faculty report that explicitly discussing AI’s proper use with students at the start of a course leads to more thoughtful use of the tools rather than sneaky cheating. In China, the strong emphasis on disclosure and formal reviews (like thesis checks) might deter students from over-relying on AI for high-stakes work, since they know it will be checked and could cost them their degree if misconduct is found ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20said%20violations%20of,obtained%20degree%20will%20be%20annulled)) ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=deduction%20in%20graduation%20thesis%20scores,obtained%20degree%20will%20be%20annulled)). The Chinese strategy of allowing AI use but within strict limits (e.g. <=40% content, and not for core arguments ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Tianjin%20University%20of%20Science%20and,and%20required%20to%20make%20revisions))) tries to channel student behavior: students can gain the efficiency benefits of AI (like faster literature surveys ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=A%20senior%20student%20at%20Communication,but%20only%20in%20some%20sections))), but must still contribute original thought. This may reduce the incentive to cheat outright. However, one could argue that setting a “40% AI content” threshold might invite some students to push right up to that line (how to quantify 40% of a text as AI-generated is also complex). **Culturally**, Chinese students and faculty may be more accustomed to compliance with detailed guidelines, whereas Western students value individual academic freedom – this can affect how rules are perceived. In the West, overly stringent rules can sometimes provoke pushback or clever circumvention; in China, open defiance of university policy is rarer given the hierarchical system. Ultimately, neither context can completely prevent misuse – integrity comes down to student ethics and enforcement. Both China and the West are learning that a combination of **education, deterrence, and honor culture** is needed.

- **Education and Support vs. Policing:** A key difference in tone is that Western discourse on AI in academia often emphasizes *not stifling innovation*. Many Western educators worry that if we only police AI, we miss an opportunity to improve pedagogy. As one technology ethicist noted, *“We don’t want witch-hunts for AI generated writing”* and we must avoid an atmosphere of suspicion that could chill legitimate use of AI for learning ([The Ethics of College Students Using ChatGPT - University Policy](https://ethicspolicy.unc.edu/news/2023/04/17/the-ethics-of-college-students-using-chatgpt/#:~:text=The%20Ethics%20of%20College%20Students,witchhunts%20for%20AI%20generated%20writing)) ([The Ethics of College Students Using ChatGPT - University Policy](https://universitypolicy.unc.edu/news/2023/04/17/the-ethics-of-college-students-using-chatgpt/#:~:text=Conversations%20between%20students%20and%20faculty,witchhunts%20for%20AI%20generated%20writing)). This has led to an approach that couples enforcement with support: e.g. providing resources on how to cite AI properly, how to fact-check AI outputs, and how to use AI for practice or drafts in an honest way ([AI & Academic Integrity | Center for Teaching Innovation](https://teaching.cornell.edu/generative-artificial-intelligence/ai-academic-integrity#:~:text=Innovation%20teaching,specific%20directions)) ([Teaching AI Ethics: Truth and Academic Integrity - Leon Furze](https://leonfurze.com/2023/03/21/teaching-ai-ethics-truth-and-academic-integrity/#:~:text=Here%E2%80%99s%20the%20original%20PDF%20infographic,tempt%20students%20to%20bypass)). In China, the tone has been more strictly protective of academic integrity – the emphasis is on **guardrails and preserving originality**. Experts like Xiong Bingqi have publicly said students *“must not rely [on AI] excessively or just copy and paste AI-generated text as their essay”* ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Xiong%20Bingqi%2C%20director%20of%20the,of%20AI%20to%20ghostwrite%20papers)) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=papers)), and that AI should only be a reference to compare against one’s own ideas ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=paper%20writing%2C%20Xiong%20said%2C%20but,of%20the%20student%27s%20independent%20research)). There is an implicit concern in China about AI undermining the hard work ethos of scholarship. Still, Chinese educators also see the value: Xiong notes “reasonable and creative use of AI can improve students’ research projects and paper writing” ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=More%20than%2080%20percent%20of,China%20Youth%20Daily%20in%20November)) – a sentiment not far from what Western proponents say. Thus, both sides converge on the idea of **ethical use**; they differ in how much freedom students are given to explore that. China’s approach is more paternalistic (explicit permission needed, or else it’s misconduct), whereas Western approach often places responsibility on the student to use good judgment within guidelines.

In summary, Western policies have been more decentralized, iterative, and pedagogically driven, while Chinese policies have been more standardized, with stronger enforcement measures linked to formal evaluation (like thesis reviews). Chinese universities benefit from alignment with national AI regulations that reinforce disclosure and control, whereas Western institutions operate in a more open AI ecosystem and must craft their own safeguards. Each approach has had some success in clarifying expectations, but both face the reality that **AI is here to stay and students are using it**. Next, we highlight some major milestones and studies that have informed these evolving governance strategies, and then consider what the future might hold for academic integrity in the age of AI.

# Landmark Events and Milestones Influencing AI Governance in Academia  

- **February 2019 – OpenAI’s GPT-2 and “Responsible Release”:** OpenAI’s decision *not* to release GPT-2’s full model initially due to misuse concerns ([This news article about the full public release of OpenAI's 'dangerous ...](https://www.theregister.com/2019/11/06/openai_gpt2_released/#:~:text=,articles%2C%20phishing%20and%20spam)) can be seen as the first warning shot about generative AI’s potential impact on information integrity. Though not aimed at education, it raised awareness in academic circles about what was coming. It also established the practice of staged releases and ethical considerations by AI developers, which continues to influence how new models (like GPT-4 with safety research) are rolled out.

- **2020 – GPT-3’s Demos (Guardian Op-Ed and Others):** The *Guardian* GPT-3 op-ed in Sept 2020 ([](https://prism.ucalgary.ca/server/api/core/bitstreams/add8f9c6-647a-44f7-b9be-be237874e496/content#:~:text=general%20public%20Fall%202020%3A%20Limited,on%20Reddit%20and%20Hacker%20News)) was a landmark in public perception – it showed that AI can produce editorial-quality prose. Academics cited this event in discussions about the future of student writing and even in scholarly papers about AI in education. It arguably set off the notion that *“the college essay is dead”* (to quote a famous Atlantic article from late 2022) because it provided a concrete example of AI-written content passing muster with editors and readers.

- **Mid 2022 – GitHub Copilot in Programming Classes:** The integration of AI code generation into widely-used tools (Copilot in Visual Studio Code) by 2022 was a wake-up call for computer science educators. Articles like *“GitHub Copilot may be perfect for cheating CompSci assignments”* gained attention ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=%3E%20Microsoft%27s%20AI%20code,of%20it%20%E2%80%93%20pointless%20because)) ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=%3E%20,computer%20science%20classes%2C%20and%20especially)). This prompted workshops and studies on how to redesign programming courses – a harbinger for what would happen with essay writing soon after.

- **Nov 2022 – Launch of ChatGPT:** ChatGPT’s release on Nov 30, 2022, is perhaps **the** landmark event for academia in this context. Its unprecedented adoption (reaching millions of users in days) made generative AI a household name on campuses ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=Fast,%E2%80%9D)). By the start of Spring 2023 semester, countless students were already using it, and faculty were exchanging strategies to respond. This moment is when academic integrity discussions left the sidelines and became mainstream in faculty meetings across the world.

- **Jan 2023 – First University Bans and Guidelines:** Within weeks of ChatGPT’s debut, we saw *the first institutional bans* (Sciences Po on Jan 27, 2023 ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=In%20an%20email%20addressed%20to,higher%20education%20as%20a%20whole)), University of Bangalore reportedly in early 2023 for its campus, HKU on Jan 27, 2023 ([University of Hong Kong issues interim ban on ChatGPT, AI-based ... - CGTN](https://news.cgtn.com/news/2023-02-19/University-of-Hong-Kong-issues-interim-ban-on-ChatGPT-AI-based-tools-1hxWzqgcMxy/index.html#:~:text=University%20of%20Hong%20Kong%20issues,on%20AI%20tools%2C%20said))). These made headlines and sparked debates on academic freedom vs. integrity. Also in January, educational media (e.g. EdSurge, Chronicle of Higher Ed) published numerous articles on how to handle AI. The media coverage served as a conduit for sharing emerging practices, like reminding everyone that *existing honor codes do cover AI misuse* ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=%E2%80%9CAs%20we%20begin%20the%20second,%E2%80%9D)). In China, around this time, major universities and even the Education Ministry likely discussed internally how to maintain standards – though Chinese universities didn’t publicly ban ChatGPT, the lack of access and early detection trials were put in motion.

- **Early 2023 – Development of AI Detection Tools:** The creation of GPTZero (Jan 2023) and OpenAI’s own detector (Jan 2023) was significant. While these tools were imperfect, they demonstrated that the tech community was also trying to help educators. Turnitin’s April 2023 update to flag AI writing (and the controversies around its accuracy) was a major event for any institution already using Turnitin – suddenly faculty had an automated way to check for AI, which some used enthusiastically and others warily. This also raised student awareness that their AI-assisted work might be scrutinized, potentially deterring casual misuse.

- **Mid 2023 – Institutional Policy Adoption and Research:** By mid-year, many universities had formally updated their academic integrity policies or published official guidance (Stanford’s in Feb 2023 ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Absent%20a%20clear%20statement%20from,such%20assistance%20when%20in%20doubt)), University of Melbourne’s recommendations in March 2023 ([CHATGPT AND ACADEMIC INTEGRITY: OPTIONS FOR ADAPTING ASSESSMENT IN ...](https://melbourne-cshe.unimelb.edu.au/__data/assets/pdf_file/0008/4533218/ChatGPT-and-Academic-Integrity.pdf#:~:text=CHATGPT%20AND%20ACADEMIC%20INTEGRITY%3A%20OPTIONS,vulnerable%20to%20plagiarism%20and%20cheating)), etc.). The spring and summer of 2023 also saw a boom in *research studies on AI in education*. For example, a preprint titled “Waiting, Banning, or Embracing: Universities’ Adapting Policies for Generative AI” analyzed the top 500 global universities and found that by May 2023, **less than one-third had any official ChatGPT policy**, and among those, about 2/3 **embraced** or allowed AI in some form, while about 1/3 banned it for assignments ([[2305.18617] Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education](https://ar5iv.org/pdf/2305.18617#:~:text=QS%20World%20University%20Rankings,having%20a%20ChatGPT%20policy%2C%20including)) ([[2305.18617] Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education](https://ar5iv.org/pdf/2305.18617#:~:text=included%20in%20the%20study%20had,In%20addition%2C%20we%20found)). Such studies provided empirical insight – for instance, they noted that English-speaking universities and those with higher rankings were more likely to have issued AI policies, and that bans often came with caveats allowing instructor discretion ([[2305.18617] Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education](https://ar5iv.org/pdf/2305.18617#:~:text=QS%20World%20University%20Rankings,having%20a%20ChatGPT%20policy%2C%20including)) ([[2305.18617] Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education](https://ar5iv.org/pdf/2305.18617#:~:text=learning%2C%20more%20than%20twice%20the,speaking%20country%20dummy)). Another influential piece of scholarship was the idea of rethinking assessments: papers in educational journals proposed strategies like more oral exams, vivas for thesis defense, and “authentic assessments” that require personal reflection or hands-on work (harder for AI to mimic). These recommendations started to shape university teaching workshops for the new academic year.

- **July 2023 – China’s Generative AI Regulations:** The finalization of China’s **Interim Measures for Generative AI** (July 2023) was a regulatory milestone ([China's New Rules For Generative AI: An Emerging Regulatory ... - Fasken](https://www.fasken.com/en/knowledge/2023/08/chinas-new-rules-for-generative-ai#:~:text=Fasken%20www,government%20agencies%2C%20including%20the)). While not specific to education, it set a precedent globally as one of the first comprehensive government rules on generative AI. It signaled to Chinese academia that the government supports AI development but within boundaries of safety and responsibility. This likely accelerated Chinese universities’ efforts to formalize their own AI guidelines in late 2023 and 2024, as we saw with Fuzhou University, Tianjin University of Science & Tech, and others implementing thesis rules ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Fuzhou%20University%20in%20Fujian%20province,be%20considered%20in%20grade%20evaluation)) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Tianjin%20University%20of%20Science%20and,and%20required%20to%20make%20revisions)). These events also contrast with the West, where by late 2023 regulators were still deliberating and had not enacted equivalent rules.

- **Late 2023 – Turn towards Integration and AI Literacy:** By the end of 2023, a notable shift occurred in many Western institutions from emergency measures to a longer-term view. Influential voices and events include the UNESCO and OECD panels on AI in education, which in late 2023 convened educators globally to share best practices. Many universities held internal symposia or task forces – for example, Sciences Po (which initially banned ChatGPT) hosted conferences on “the future of education in an AI world” as part of its TIERED project ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=As%20an%20institution%20that%20encourages,playing%20an%20increasingly%20significant%20role)). The conversation broadened from just policing cheating to also *harnessing AI for education*. A major theme was **AI literacy**: ensuring students understand how AI works, its limitations (like making up facts ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=There%20are%20other%20significant%20limitations,This%20is))), and the ethical issues around it. The idea is that by incorporating AI literacy into curricula, students will be less likely to misuse it and more likely to use it as a complement to their own skills. This mirrors how digital literacy (e.g. how to responsibly use the internet for research) became part of education in earlier decades to combat copy-paste plagiarism.

Each of these milestones – technological, institutional, and regulatory – has incrementally shaped the governance of AI in academic settings. The result as we stand in 2024 is a more informed and prepared academic community, but one that is still feeling its way through a fast-evolving landscape.

# Future Implications and Conclusion  

The intersection of generative AI and academic integrity will continue to evolve in the coming years, and institutions will need to remain agile. **Looking ahead**, several implications and strategies can be anticipated:

- **Continual Advancements in AI:** The pace of AI progress shows no sign of slowing. Forthcoming models (e.g. the hypothetical “GPT-5” or new multimodal systems) may produce even more indistinguishable text, solve complex scientific problems, or generate full multimedia content. AI assistance will likely be integrated into common software – for example, Microsoft has announced plans to embed GPT-4 based copilot features in Office products (Word, Outlook, etc.), meaning students could have AI writing suggestions by default as they compose documents. This ubiquity will blur the line between where a student’s own effort ends and AI’s help begins. **Academic policies will need to shift from reactive to proactive**, establishing what “authorized use” of built-in AI tools means. It may become impractical to ban AI when every word processor has an AI mode; instead, education might focus on attribution (perhaps documents will have an “AI contributions” log). Schools might lobby software makers to include an “AI use disclosure” feature for transparency.

- **Redefining Assessment and Skills:** To maintain meaningful assessments, educators will increasingly design assignments that leverage human strengths unreproducible by AI – for instance, personal reflections, experiential projects, or oral defenses. Exams may focus more on higher-order thinking and creativity, areas where human students can still outperform routine AI responses. If factual recall and generic essays are handled by AI, curricula might pivot to emphasize **critical thinking, source evaluation, and ethical reasoning**. Some experts suggest we may eventually treat AI as a calculator – a permitted aid for doing lower-level work, while grading students on how they formulate problems and interpret AI-assisted results. This could actually improve learning if done right, as students would spend more time on analysis and less on rote writing. However, achieving this will require deep pedagogical reforms and training teachers to incorporate AI effectively.

- **Strengthening Academic Integrity Culture:** The emergence of AI doesn’t eliminate the need for an academic integrity culture – it makes it more important. Honor codes might be modernized to explicitly include AI (many have done so already) and to articulate *why* integrity matters in the age of AI (e.g. emphasizing learning over mere task completion). Both in China and the West, there may be a push to instill values of honesty and pride in one’s own work from an early stage, so that students internalize why submitting AI’s work as theirs is wrong beyond just fear of punishment. Some universities are shifting to a more mentorship-based approach to misconduct: if a student is found to have used AI improperly, the response could include an educational module on AI ethics, not just a penalty. The goal is to turn a temptation into a teachable moment.

- **Improved Detection and Verification:** On the technology side, we can expect continued development of detection tools – possibly even AI systems that can analyze writing style changes over time for each student (although this raises privacy and profiling concerns). There is also research into having AI systems watermark their outputs in invisible ways; OpenAI has explored cryptographic watermarks for GPT output, which if implemented, could allow algorithms to later detect if text came from that AI. If effective, such measures might give educators a reliable way to verify authenticity. However, one must also plan for the scenario that detection becomes infeasible as AI and humans intermingle their work. In that case, verification might rely more on oral cross-checks (e.g., discussing the submitted work with the student to ensure they understand it deeply) or on process-based assessment (monitoring drafts and evolution of work to see the student’s hand in it).

- **Global Collaboration on Governance:** The challenge of AI in education is global, and there will likely be more international collaboration on solutions. UNESCO and other bodies are already working on guidelines for “AI in Education” that balance innovation and integrity. We may see the development of international standards or certifications for AI-use policies in academia, helping institutions benchmark their approaches. Western and Chinese institutions might share best practices in forums (noting that despite geopolitical differences, academia often collaborates on common issues). It’s conceivable that in the near future, major academic conferences will have tracks on AI ethics in education, and journals will publish more research evaluating what policies work best. The comparative insights – like how disclosure requirements in China affect student behavior versus how honor-code pledges in the US do – will be valuable data to refine approaches everywhere.

In conclusion, generative AI represents a profound technological shift that is testing the adaptability of academic integrity frameworks. From 2018 to the present, we have moved from early fears that AI would *kill* academic writing to more nuanced efforts to *govern* AI use responsibly. Major AI advancements (GPT-2, GPT-3, GPT-4, etc.) have successively raised the stakes, and institutions have responded with a mix of restrictive and adaptive strategies. Western universities initially scrambled to contain the “cheating epidemic” but are increasingly focusing on integrating AI literacy and updating pedagogy. Chinese universities, guided by national policy, have implemented strict rules to prevent misconduct while still leveraging AI’s benefits under supervision. Both approaches contribute pieces to the puzzle of maintaining integrity. Key regulatory and policy milestones – from university bans to government measures – have set precedents, and influential studies are shedding light on effective practices. 

Academic integrity has always evolved alongside technology (from calculators to the internet), and generative AI is perhaps its greatest stress test yet. The experience since 2018 shows the importance of **agility and clarity**: when policies are clear and grounded in academic values, students respond more positively ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=But%20not%20all%20students%20and,who%20disagreed)); when educators adapt assessments creatively, they can often stay one step ahead of AI misuse. The future will undoubtedly bring more sophisticated AI, but also better understanding of how humans learn with AI. By fostering an environment where AI is viewed not as a shortcut to cheat, but as a tool that comes with ethical responsibilities, academic institutions can uphold integrity and even enrich the learning process. In the end, preserving academic integrity in the age of AI will require what it always has: a community commitment to honesty, continuous dialogue between instructors and students, and the ability to evolve our methods while holding fast to our educational mission.

**Sources:**

1. BDO Nonprofit Education Report – Generative AI timeline and impact ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=available%20online%20artificial%20intelligence%20,the%20tool%E2%80%99s%20accuracy%20and%20use)) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=Fast,%E2%80%9D)).  
2. BDO Report – Prevalence of student AI use and survey data ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=A%20poll%20of%20Stanford%20University,and%20outside%20of%29%20academia)) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=But%20not%20all%20students%20and,who%20disagreed)).  
3. University of Chicago Academic Tech Blog – AI evading plagiarism checkers ([Combating Academic Dishonesty, Part 6: ChatGPT, AI, and Academic Integrity | Academic Technology Solutions](https://academictech.uchicago.edu/2023/01/23/combating-academic-dishonesty-part-6-chatgpt-ai-and-academic-integrity/#:~:text=It%20is%2C%20without%20question%2C%20too,existing%20work%20and)).  
4. China Daily (Zhao, 2024) – Chinese universities regulating AI in theses (Fuzhou U, Tianjin UST) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Fuzhou%20University%20in%20Fujian%20province,be%20considered%20in%20grade%20evaluation)) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=More%20than%2080%20percent%20of,China%20Youth%20Daily%20in%20November)).  
5. China Daily (Dec 2024) – Fudan University’s detailed AI thesis policy ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20announced%20in%20its,writing%20process)) ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20stipulation%20clearly%20stated%20that,theses%20were%20completed%20using%20them)).  
6. Sciences Po Press Release (Jan 2023) – ChatGPT ban and rationale ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=In%20an%20email%20addressed%20to,higher%20education%20as%20a%20whole)) ([Sciences Po bans the use of ChatGPT without transparent referencing | Newsroom Sciences Po | Espace Presse](https://newsroom.sciencespo.fr/sciences-po-bans-the-use-of-chatgpt#:~:text=Sciences%20Po%E2%80%99s%20senior%20management%20team,higher%20education%20as%20a%20whole)).  
7. Stanford Honor Code Guidance (Feb 2023) – Default rules for generative AI use ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Absent%20a%20clear%20statement%20from,such%20assistance%20when%20in%20doubt)) ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Students%20should%20acknowledge%20the%20use,such%20assistance%20when%20in%20doubt)).  
8. EdSurge (Jan 2023) – “Colleges in emergency mode” & honor code letter excerpt ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=%E2%80%9CAs%20we%20begin%20the%20second,%E2%80%9D)).  
9. Business Insider (Jan 2023) – Wharton professor requiring AI use as “emerging skill” ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=Ethan%20Mollick%2C%20an%20entrepreneurship%20and,to%20help%20with%20their%20classwork)) ([A Wharton business school professor is requiring his students to use ChatGPT](https://www.businessinsider.nl/a-wharton-business-school-professor-is-requiring-his-students-to-use-chatgpt/#:~:text=His%20new%20AI%20policy%20%E2%80%94,that%20the%20bot%20spits%20out)).  
10. The Register (Aug 2022) – Concerns about GitHub Copilot in CS education ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=%3E%20Microsoft%27s%20AI%20code,of%20it%20%E2%80%93%20pointless%20because)) ([GitHub Copilot may be perfect for cheating CompSci programming exercises - Page 1](https://www.eevblog.com/forum/general-computing/github-copilot-may-be-perfect-for-cheating-compsci-programming-exercises/#:~:text=%3E%20,computer%20science%20classes%2C%20and%20especially)).  
11. ArXiv Study (Xiao et al., 2023) – Analysis of global university ChatGPT policies ([[2305.18617] Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education](https://ar5iv.org/pdf/2305.18617#:~:text=QS%20World%20University%20Rankings,having%20a%20ChatGPT%20policy%2C%20including)) ([[2305.18617] Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education](https://ar5iv.org/pdf/2305.18617#:~:text=included%20in%20the%20study%20had,In%20addition%2C%20we%20found)).  
12. China Daily (Dec 2024) – Example of paper retraction for undisclosed AI use ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=In%20March%2C%20a%20research%20paper,prompter%20in%20the%20article%27s%20introduction)).  
13. ICML 2023 Policy – Ban on AI-generated text in research paper submissions ([AI conference and NYC's educators ban papers done by ChatGPT](https://www.theregister.com/2023/01/06/ai_conference_nyc_ban/#:~:text=People%20are%20increasingly%20using%20them,the%20paper%27s%20experimental%20analysis)) ([AI conference and NYC's educators ban papers done by ChatGPT](https://www.theregister.com/2023/01/06/ai_conference_nyc_ban/#:~:text=,the%20paper%27s%20experimental%20analysis)).  
14. Stanford Daily Poll via BDO – 17% Stanford students used AI (Fall 2022) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=A%20poll%20of%20Stanford%20University,and%20outside%20of%29%20academia)).  
15. BestColleges Student Survey via BDO – 43% used AI; attitudes on cheating ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=complete%20homework,and%20outside)) ([A Series on Generative AI: Is Academic Integrity Gone for Good? | BDO](https://www.bdo.com/insights/blogs/nonprofit-standard/a-series-on-generative-ai-is-academic-integrity-a-thing-of-the-past#:~:text=use%20of%20chatbots,exams%2C%E2%80%9D%20more%20than%20twice%20the)).  
16. China Youth Daily survey via China Daily – 80% Chinese students used AI ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=essay%20content%2C)).  
17. Xiong Bingqi’s comments on AI use in China – allow but not for ghostwriting ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=Xiong%20Bingqi%2C%20director%20of%20the,of%20AI%20to%20ghostwrite%20papers)) ([Universities regulate use of AI in writing | govt.chinadaily.com.cn](https://govt.chinadaily.com.cn/s/202406/11/WS66740da3498ed2d7b7eb0d1b/universities-regulate-use-of-ai-in-writing.html#:~:text=paper%20writing%2C%20Xiong%20said%2C%20but,of%20the%20student%27s%20independent%20research)).  
18. Texas State Univ. Honor Council letter excerpt via EdSurge ([ChatGPT Has Colleges in Emergency Mode to Shield Academic Integrity | EdSurge News](https://www.edsurge.com/news/2023-01-24-chatgpt-has-colleges-in-emergency-mode-to-shield-academic-integrity#:~:text=%E2%80%9CAs%20we%20begin%20the%20second,%E2%80%9D)).