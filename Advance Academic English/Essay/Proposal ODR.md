# A Comparative Analysis of Governance Frameworks for Upholding Academic Integrity in Essay Writing with Generative AI

## Introduction & Literature Review

Generative AI tools like OpenAI’s ChatGPT have rapidly entered the academic scene, raising new questions about academic integrity in essay writing. Launched in late 2022, ChatGPT can produce coherent essays within minutes ([The Benefits, Risks and Regulation of Using ChatGPT in Chinese Academia: A Content Analysis](https://www.mdpi.com/2076-0760/12/7/380#:~:text=popularized%20worldwide%20rapidly%20,an%20act%20of%20cheating%20that)), blurring the line between a student’s own work and AI-generated content. Its uptake among students has been swift: for example, a recent survey in China found nearly 60% of university faculty and students use generative AI at least weekly, with about 30% of students relying on AI to help write assignments ([Chinese universities tighten AI usage rules to curb academic misuse](http://www.ecns.cn/news/cns-wire/2025-02-27/detail-ihepcskv6108382.shtml#:~:text=A%20survey%20by%20MyCOS%2C%20an,a%20CCTV%20news%20portal%20reported)). In the United States, an informal poll at Stanford University similarly revealed a significant number of students admitted to using ChatGPT for final assessments ([](https://en.front-sci.com/index.php/rerr/article/view/2164/2368#:~:text=research%20,constitutes%20academic%20misconduct%20is%20currently)). This widespread adoption underscores the pressing need to address how such tools can be used ethically. Educational institutions now face the challenge of preserving academic honesty when students have access to AI that can generate passable (even excellent) essays on demand.

The rise of generative AI poses several **challenges to academic integrity**. One major concern is plagiarism: students may submit AI-written text as if it were their own work, undermining the principle of original scholarship ([The Benefits, Risks and Regulation of Using ChatGPT in Chinese Academia: A Content Analysis](https://www.mdpi.com/2076-0760/12/7/380#:~:text=academic%20responsibilities%20has%20been%20polarized,contained%20more%20positive%20and%20negative)). Because ChatGPT’s output is often fluent and well-structured, detecting AI-generated cheating can be extremely difficult – experts note that purely examining an essay’s writing style may not reveal its true origin ([AI makes plagiarism harder to detect, argue academics – in paper written by chatbot | Chatbots | The Guardian](https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot#:~:text=Thomas%20Lancaster%2C%20a%20computer%20scientist,said%20many%20universities%20were%20%E2%80%9Cpanicking%E2%80%9D)). Additionally, AI systems have been known to fabricate sources and citations, producing realistic-looking but nonexistent references ([AI makes plagiarism harder to detect, argue academics – in paper written by chatbot | Chatbots | The Guardian](https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot#:~:text=Nonetheless%2C%20he%20said%20academics%20could,or%20makes%20them%20up%20completely)). This jeopardizes the integrity of academic work, as essays could include false evidence that evades standard plagiarism checks. Beyond plagiarism, educators worry that over-reliance on AI hinders learning: if students bypass the cognitive effort of writing and critical thinking, they miss out on developing those skills. In sum, generative AI enables new forms of academic misconduct (from undetectable ghostwriting to faked data or sources) that existing academic integrity policies struggle to cover.

Universities and authorities are beginning to respond. **In China, educational authorities and institutions have taken an active stance**. China’s Ministry of Education has issued directives emphasizing vigilance against AI-related academic misconduct ([](https://en.front-sci.com/index.php/rerr/article/view/2164/2368#:~:text=a%20heated%20topic%20in%20the,suggested%20that%20based%20on%20the)). Leading Chinese universities have swiftly enacted stringent policies to **govern AI usage in coursework**. For instance, Fudan University recently introduced comprehensive regulations banning the use of AI in *six key areas* of undergraduate thesis work – including research design, data analysis, result generation, and writing of the thesis itself ([Chinese universities tighten AI usage rules to curb academic misuse](http://www.ecns.cn/news/cns-wire/2025-02-27/detail-ihepcskv6108382.shtml#:~:text=In%20response%2C%20leading%20universities%2C%20including,what%20is%20permitted%20and%20prohibited)). Under Fudan’s rules, students are *not* allowed to use ChatGPT or similar tools to draft any part of the thesis text or to create figures and data, and they must explicitly declare any AI assistance they did receive ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20announced%20in%20its,writing%20process)) ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20stipulation%20clearly%20stated%20that,theses%20were%20completed%20using%20them)). Permissible uses (such as literature search or coding help) are tightly controlled and require faculty approval ([Chinese universities tighten AI usage rules to curb academic misuse](http://www.ecns.cn/news/cns-wire/2025-02-27/detail-ihepcskv6108382.shtml#:~:text=Students%20are%20allowed%20to%20use,failing%20grades%20or%20degree%20revocation)). The penalties for violations are severe: students may receive failing grades or even have their degrees revoked for improper AI use ([Chinese universities tighten AI usage rules to curb academic misuse](http://www.ecns.cn/news/cns-wire/2025-02-27/detail-ihepcskv6108382.shtml#:~:text=Students%20are%20allowed%20to%20use,failing%20grades%20or%20degree%20revocation)). Such strict measures reflect a **strength** of the Chinese approach – clear boundaries and strong deterrence – but also a potential **shortcoming**, as overly rigid bans might discourage legitimate educational use of AI. Indeed, some Chinese policies are experimenting with balance; for example, Tianjin University allows up to 40% of an undergraduate thesis to be AI-generated (as detected by software) to encourage transparency without outright prohibition ([Chinese universities tighten AI usage rules to curb academic misuse](http://www.ecns.cn/news/cns-wire/2025-02-27/detail-ihepcskv6108382.shtml#:~:text=At%20Tianjin%20University%20of%20Science,in%20effect%20for%202025%20graduates)). Overall, China’s governance framework is characterized by proactive, top-down rules that underscore integrity (sometimes at the expense of flexibility).

**In the United States, responses have been more decentralized and evolving.** There is no federal-level policy on AI in student work, so governance largely comes from universities. Many American institutions initially reacted cautiously in the absence of established guidelines. Stanford University was among the first to address the issue through its Honor Code framework. In early 2023, Stanford’s Board on Conduct Affairs issued guidance stating that using generative AI to complete assignments should be treated equivalently to receiving inappropriate help from a person – in other words, having ChatGPT write an essay for you would violate the Honor Code ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Absent%20a%20clear%20statement%20from,such%20assistance%20when%20in%20doubt)). Students are expected to **acknowledge any use of AI** beyond trivial assistance, and using AI to substantially answer exam or essay questions is explicitly prohibited ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Absent%20a%20clear%20statement%20from,such%20assistance%20when%20in%20doubt)). At the same time, Stanford’s policy **allows individual instructors discretion**: professors are “free to set their own policies” on AI usage in their courses and syllabi, whether more permissive or more strict ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Individual%20course%20instructors%20are%20free,ask%20their%20instructors%20for%20clarification)). This approach encourages experimentation in teaching while maintaining a baseline of academic integrity. Harvard University took a similar route of guidance over mandate. By the 2023–24 academic year, Harvard’s Office of Undergraduate Education had provided faculty with several model policies for syllabi, ranging from a **“maximally restrictive” rule (completely banning AI)** to a fully permissive stance integrating AI tools into coursework ([‘Struggling to Keep Up’: Harvard Students and Faculty Grapple with Impact of Generative AI in Classrooms | News | The Harvard Crimson](https://www.thecrimson.com/article/2023/9/22/generative-ai-policies-students-react/#:~:text=Though%20the%20Office%20of%20Undergraduate,generative%20AI%20in%20their%20courses)). Each professor could adopt the policy that best fit their course, and many courses began explicitly addressing AI in their honor code statements. Some Harvard classes chose to ban generative AI for any graded work – for example, one government course warned that *all submitted work must be the student’s own, and any AI involvement would count as academic misconduct* ([‘Struggling to Keep Up’: Harvard Students and Faculty Grapple with Impact of Generative AI in Classrooms | News | The Harvard Crimson](https://www.thecrimson.com/article/2023/9/22/generative-ai-policies-students-react/#:~:text=Some%20instructors%20are%20taking%20a,stages%20of%20the%20writing%20process)). On the other hand, a few instructors encouraged responsible use of tools like ChatGPT for brainstorming or drafting, provided students cited the AI’s contributions. This variation in U.S. institutions’ policies highlights a **strength** – flexibility and innovation in pedagogy – but also reveals **shortcomings**. In the early months of ChatGPT’s emergence, universities struggled to keep up; the lack of a timely, unified policy at places like Harvard drew criticism that institutions were lagging behind the technology ([The (Mis)Education of ChatGPT | Opinion | The Harvard Crimson](https://www.thecrimson.com/article/2023/2/3/editorial-chatgpt-education/#:~:text=the%20current%20lack%20of%20response,appears%20to%20be%20woefully%20unprepared)). Indeed, an editorial by *The Harvard Crimson* called the administration “woefully unprepared” in early 2023 and argued that blanket bans on AI would be counterproductive ([The (Mis)Education of ChatGPT | Opinion | The Harvard Crimson](https://www.thecrimson.com/article/2023/2/3/editorial-chatgpt-education/#:~:text=Speaking%20at%20a%20webinar%20at,on%20ChatGPT%20does%20not%20suffice)) ([The (Mis)Education of ChatGPT | Opinion | The Harvard Crimson](https://www.thecrimson.com/article/2023/2/3/editorial-chatgpt-education/#:~:text=the%20current%20lack%20of%20response,appears%20to%20be%20woefully%20unprepared)). Over 2023, as awareness grew, most top U.S. universities updated their academic integrity guidelines to address generative AI, but the pace and strictness of these updates varied widely.

Parallel to policy responses, **scholarly research on AI and academic integrity has quickly expanded**. Initial literature often highlighted theoretical concerns or opinion-based commentary, given the recency of tools like ChatGPT. More recently, empirical studies have begun to shed light on actual usage patterns and impacts. For example, *Hung and Chen (2023)* conducted a content analysis of Chinese media discussions about ChatGPT in education ([The Benefits, Risks and Regulation of Using ChatGPT in Chinese Academia: A Content Analysis](https://www.mdpi.com/2076-0760/12/7/380#:~:text=eligible%20for%20data%20analysis,in%20Chinese%20academic%20settings%20neutrally)). They found views in China were polarized: *conservative voices* worry that students will use AI to cheat, while *optimistic educators* suggest generative AI can be harnessed to improve writing and learning outcomes ([The Benefits, Risks and Regulation of Using ChatGPT in Chinese Academia: A Content Analysis](https://www.mdpi.com/2076-0760/12/7/380#:~:text=academic%20responsibilities%20has%20been%20polarized,contained%20more%20positive%20and%20negative)). Notably, this study identified plagiarism as the chief concern among Chinese educators regarding AI ([The Benefits, Risks and Regulation of Using ChatGPT in Chinese Academia: A Content Analysis](https://www.mdpi.com/2076-0760/12/7/380#:~:text=Chinese%20educators%20believe%20AI,in%20Chinese%20academic%20settings%20neutrally)), confirming that unauthorized AI-written work is viewed as a serious form of academic misconduct. In Western contexts, researchers like Cotton et al. (2023) have demonstrated the capabilities of AI and the resultant dilemmas by, for instance, publishing an academic article written largely by ChatGPT to prove how difficult it is to tell apart from human writing ([AI makes plagiarism harder to detect, argue academics – in paper written by chatbot | Chatbots | The Guardian](https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot#:~:text=An%20academic%20paper%20entitled%20Chatting,to%20academic%20honesty%20and%20plagiarism%E2%80%9D)) ([AI makes plagiarism harder to detect, argue academics – in paper written by chatbot | Chatbots | The Guardian](https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot#:~:text=Thomas%20Lancaster%2C%20a%20computer%20scientist,said%20many%20universities%20were%20%E2%80%9Cpanicking%E2%80%9D)). Such works emphasize that traditional plagiarism detection methods falter with AI-generated text and call for new strategies (such as focusing on oral defenses, process-based assessments, or AI detection tools). Studies have also started measuring attitudes and prevalence: surveys indicate a significant minority of students admit to using AI in coursework, often without formal permission ([](https://en.front-sci.com/index.php/rerr/article/view/2164/2368#:~:text=research%20,constitutes%20academic%20misconduct%20is%20currently)), while faculty express mixed feelings – some see it as a threat to honesty, others as a potential aide if used transparently. Importantly, **recent research on institutional policies** has employed innovative methods to compare how different universities are adapting. One cross-national review examined guidelines from top universities worldwide and noted a spectrum of strategies from outright bans to full integration ([Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration | International Journal of Educational Technology in Higher Education | Full Text](https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-025-00507-3#:~:text=higher%20education%20,23)). In a U.S.-focused analysis, An, Yu, and James (2025) gathered AI policy statements from the 50 leading American universities and used text-mining techniques to find common themes ([Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration | International Journal of Educational Technology in Higher Education | Full Text](https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-025-00507-3#:~:text=This%20study%20examined%20the%20guidelines,revealed%20highly%20positive%20attitudes%20towards)). They discovered that nearly all these universities now urge faculty to set clear course-specific AI rules, and **“academic integrity” has emerged as a dominant theme** across policies ([Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration | International Journal of Educational Technology in Higher Education | Full Text](https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-025-00507-3#:~:text=through%20sentiment%20analysis%2C%20which%20revealed,GenAI%20guidelines%20in%20higher%20education)). Interestingly, that study found generally positive or hopeful framing of AI’s role, coupled with cautionary notes about ethics and misuse ([Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration | International Journal of Educational Technology in Higher Education | Full Text](https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-025-00507-3#:~:text=comprehensive%20understanding%20of%20institutional%20responses,the%20importance%20of%20establishing%20and)) ([Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration | International Journal of Educational Technology in Higher Education | Full Text](https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-025-00507-3#:~:text=through%20sentiment%20analysis%2C%20which%20revealed,identified%20in%20the%20topic%20modeling)). This suggests that institutions are not only reacting punitively but also looking for ways to responsibly incorporate AI. 

Given the fast-paced developments, researchers are also examining **methodological approaches** for studying the AI-policy landscape. **Corpus-based policy analysis** has proven especially valuable for identifying dominant narratives and tracking trends over time. By treating collections of policy documents, honor codes, or guidelines as textual data, one can apply content analysis or computational linguistics techniques to reveal patterns that might not be evident anecdotally. For example, topic modeling on the corpus of AI policies helped highlight which concerns (e.g. integration in learning vs. misconduct prevention) are most prevalent and how sentiment toward AI tools differs between audiences ([Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration | International Journal of Educational Technology in Higher Education | Full Text](https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-025-00507-3#:~:text=comprehensive%20understanding%20of%20institutional%20responses,the%20importance%20of%20establishing%20and)) ([Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration | International Journal of Educational Technology in Higher Education | Full Text](https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-025-00507-3#:~:text=through%20sentiment%20analysis%2C%20which%20revealed,GenAI%20guidelines%20in%20higher%20education)). Likewise, longitudinal analysis of policy documents can show how the language around “academic integrity” and “AI” evolves as institutions refine their stances. Such corpus-based analyses complement traditional qualitative reviews by adding a systematic, big-picture view of the discourse. In summary, existing literature and preliminary data underscore that the **governance frameworks** for AI in academia are in flux – Chinese and U.S. institutions are taking markedly different approaches, each with their pros and cons, and there is a need for deeper comparative study grounded in both policy analysis and stakeholder insights.

## Research Objectives & Questions

This proposed study aims to **compare and evaluate the governance frameworks** in China and the United States for upholding academic integrity in the era of generative AI-assisted essay writing. By examining policies and practices across these two contexts, the research seeks to identify what each side might learn from the other and to pinpoint gaps between official guidelines and actual use on the ground. Ultimately, the study will offer recommendations to enhance academic integrity governance in both countries, ensuring that policies neither unduly stifle innovation nor allow academic dishonesty to proliferate.

**Central Research Questions:**

1. **What policies and guidelines exist in Chinese and U.S. higher education to govern the use of generative AI (e.g. ChatGPT) in essay writing, and what are the key features of these governance frameworks?**  
   *This question will catalog and compare the official stances – from government directives and university regulations to honor code provisions – highlighting similarities and differences in approach.*

2. **How do these governance frameworks address the specific academic integrity challenges posed by generative AI, such as plagiarism, misrepresentation of authorship, and verification of sources?**  
   *This explores the content of policies in detail: e.g. do they ban AI outright or allow conditional use? What penalties or detection measures are outlined? Are concepts like disclosure of AI assistance or limits on AI-generated content included?*

3. **What are the perceptions and behaviors of students regarding the use of AI in their essay writing, and to what extent are students aware of or aligned with their institution’s policies in China versus the U.S.?**  
   *This question brings in the stakeholder perspective, probing whether there is a gap between policy and practice. It will help assess if current policies are realistic and known to those expected to follow them.* 

4. **In what ways do the Chinese and U.S. approaches succeed or fall short in upholding academic integrity, and what improvements or best practices can be derived from a comparative analysis?**  
   *This synthetic question will drive the discussion of implications: identifying strengths (e.g. clarity, enforcement, flexibility, educational value) and weaknesses (e.g. over-restrictiveness, lack of awareness, inconsistency) in each context’s framework, with an eye toward recommending more effective governance strategies.*

## Methods

To address these questions, the study will employ a **mixed-methods research design** that combines qualitative and quantitative approaches. This design ensures a comprehensive analysis by triangulating data from policy documents, scholarly literature, and student experiences. The methodology consists of three main components:

- **Corpus-Based Policy Analysis:** A collection (corpus) of relevant documents will be compiled for each country. For China, this will include Ministry of Education directives, university regulations (e.g. official university policy documents from Fudan, Tsinghua, etc.), and published guidelines related to AI and academic integrity. For the U.S., the corpus will include university honor codes, official guidelines from top universities (e.g. Harvard’s faculty guidance, Stanford’s Honor Code memo), and any statements from accreditation bodies or education departments on AI use. Using content analysis and basic text-mining techniques, the study will identify dominant themes and language in these documents. Keywords related to academic integrity (e.g. “plagiarism,” “cheating,” “originality”) and AI (e.g. “ChatGPT,” “AI tools”) will be analyzed to see how frequently and in what context they co-occur. The analysis will also note policy specifics (like disclosed allowed uses of AI, requirements for citation of AI, or penalties defined). By systematically comparing the Chinese and U.S. document corpora, we can highlight differences in governance approaches – for example, a Chinese policy might stress “forbidden usage” and punitive measures ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20said%20violations%20of,obtained%20degree%20will%20be%20annulled)), whereas a U.S. policy might emphasize “faculty discretion” and student responsibility ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Individual%20course%20instructors%20are%20free,ask%20their%20instructors%20for%20clarification)). The corpus analysis will thus provide an evidence-based mapping of the governance frameworks in each region.

- **Student Survey (Cross-Cultural):** We will design and administer a survey targeting university students in both China and the United States. The survey will gather data on students’ *perceptions, attitudes, and self-reported behaviors* regarding generative AI in their coursework. Key items will include: frequency of AI use for writing assignments; specific ways AI is used (idea generation, actual drafting, editing, etc.); understanding of what their university’s policy allows or forbids; and personal opinions on whether using AI counts as cheating. The survey will also quiz students on their awareness of institutional policies (e.g. “Are you aware of any official guidelines from your university about AI use in assignments? If yes, what is allowed/prohibited?”). We plan to distribute the survey online to at least two universities in each country (for instance, one top-tier and one regional university in each context, to capture some diversity). Responses will be analyzed quantitatively (descriptive statistics of AI usage rates, etc.) and qualitatively (open-ended questions about why students do or don’t use AI, ethical views, etc.). By comparing survey results between Chinese and American student samples, we can ascertain whether policy differences correlate with different student behaviors. For example, do Chinese students report less AI usage due to stricter rules, or do they use it surreptitiously? Do U.S. students feel more ambiguity about what is allowed? The survey data will shed light on the “on-the-ground” reality that governance frameworks are trying to regulate.

- **Triangulation and Comparative Analysis:** In the final stage, we will **triangulate** the findings from the document analysis and the survey to answer our research questions holistically. Triangulation means that we will look for points of agreement or discrepancy between what the policies *intend* and what students *perceive or do*. For instance, if many U.S. students are unaware their university even has an AI policy (a possible outcome), that indicates a gap in communication, no matter how well-crafted the policy may be. If Chinese students overwhelmingly know about and follow strict rules, that might indicate effective enforcement – or perhaps fear of punishment – shaping behavior. We will also compare the **Chinese vs. U.S. context directly**: identifying themes that appear in one country’s policies but not the other’s, and understanding how cultural or educational context might explain the contrast. The mixed-methods data allows us to see not just the *content* of governance frameworks, but also their *effectiveness* and reception. For credibility, we will validate our interpretations by checking for consistency (do the qualitative comments align with quantitative trends? do the policy texts explain observed student attitudes?). This triangulated approach will ultimately enable us to formulate well-grounded conclusions about what elements of governance are working, what aren’t, and why. All data collection will be conducted with ethical approval and informed consent (for the student survey), ensuring confidentiality and cultural sensitivity, especially when surveying in different languages (the survey will be prepared in both English and Chinese).

## Significance and Implications

This research will be significant for educators, policymakers, and academic institutions grappling with the fast-evolving landscape of AI in education. By providing a **comparative analysis of China and the U.S.,** the study will illuminate how different educational systems are addressing a common challenge. This East-West comparison is particularly valuable: China’s centralized, policy-driven approach and the U.S.’s decentralized, honor-code-based approach offer two distinct models for managing academic integrity in the AI era. The findings could help each side learn from the other – for example, U.S. universities might consider adopting some of the clarity and comprehensiveness seen in Chinese policies ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20announced%20in%20its,writing%20process)) ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20stipulation%20clearly%20stated%20that,theses%20were%20completed%20using%20them)), while Chinese institutions might benefit from the flexibility and educational focus seen in some American frameworks (such as integrating AI literacy into curricula rather than simply banning tools) ([The (Mis)Education of ChatGPT | Opinion | The Harvard Crimson](https://www.thecrimson.com/article/2023/2/3/editorial-chatgpt-education/#:~:text=Speaking%20at%20a%20webinar%20at,on%20ChatGPT%20does%20not%20suffice)) ([‘Struggling to Keep Up’: Harvard Students and Faculty Grapple with Impact of Generative AI in Classrooms | News | The Harvard Crimson](https://www.thecrimson.com/article/2023/9/22/generative-ai-policies-students-react/#:~:text=Though%20the%20Office%20of%20Undergraduate,generative%20AI%20in%20their%20courses)).

**Implications for policy-making:** The study’s insights will inform better policy-making by highlighting best practices and pitfalls. If the research finds that overly strict prohibitions (like blanket bans with harsh penalties) are ineffective – say, if students find workarounds or if such policies drive the behavior underground – then policy-makers can reconsider a more nuanced approach. Conversely, if clear guidelines and required disclosure of AI use are found to maintain integrity without hindering learning, more institutions might adopt those measures. The goal is to recommend governance frameworks that **uphold academic integrity while adapting to technological change**. This could include suggestions such as: requiring students to credit AI assistance (much as they would cite a source), developing honor code addendums specifically for AI, training faculty to design assessments less prone to AI cheating, and creating channels for students to discuss ethical AI use openly. For Chinese universities, implications might involve updating any one-size-fits-all bans to allow some supervised educational use of AI, thereby preventing students from being disadvantaged or unprepared for AI-pervasive workplaces. For American universities, the study might underscore the importance of institution-wide policies or honor code revisions to avoid confusion, ensuring that every student receives a consistent message about AI use and integrity.

**Academic contributions:** Beyond practical policy advice, the study will contribute to the academic discourse on ethics in technology-enhanced learning. It will add empirical data to nascent research on AI in academic integrity – especially by bringing in student perspectives which are currently under-documented. The comparative aspect can also enrich theory on how cultural and regulatory differences impact the implementation of academic integrity principles. Are certain principles (like the value of original work) universal, or enforced differently due to societal context? We expect to identify areas where the concept of “academic integrity” itself might be evolving – for instance, does accepting AI as a tool (when properly attributed) redefine what is considered legitimate assistance in learning? The study’s findings could prompt further scholarship on aligning educational values with the realities of AI tools.

In summary, the implications of this research are far-reaching. As generative AI becomes ever more sophisticated and accessible, educational institutions worldwide will need robust governance frameworks to ensure that *learning* – not just credentialing – remains at the heart of student work. By examining how China and the U.S. are tackling this issue, this study will provide timely guidance on fostering a culture of integrity and honesty in an age where the very act of writing an essay has been redefined by AI. The recommendations stemming from this work could help universities craft policies that deter misconduct, encourage ethical use of technology, and ultimately prepare students to be conscientious scholars and professionals in the AI-augmented future.

## Timeline

-   **Week 1:** Finalize research design, survey questionnaire, and text analysis plan. Initiate ethics approval process. Begin comprehensive literature review and start gathering the corpus of policy documents from Chinese and U.S. sources.
-   **Week 2:** Continue assembling and organizing the policy document corpus. Program the student survey, conduct piloting, and refine the instrument based on feedback. Continue outreach to target universities/student groups for survey participation.
-   **Week 3:** Launch the online student survey. Begin initial content analysis and coding of the policy document corpus. Monitor survey responses and send initial reminders.
-   **Week 4:** Continue survey administration, actively managing participation and sending further reminders. Intensify analysis of the policy document corpus, identifying key themes and running preliminary text analyses.
-   **Week 5:** Close the student survey. Begin quantitative analysis of survey data (statistical summaries, comparisons) and qualitative analysis of open-ended responses (thematic coding). Finalize the analysis of the policy corpus.
-   **Week 6:** Complete the analysis of survey data. Conduct triangulation by integrating findings from the policy analysis and the student survey. Begin drafting the core sections of the research report: Findings, Discussion, and Methods.
-   **Week 7:** Draft the remaining sections: Introduction (refined), Conclusion, and Recommendations. Revise the entire report for coherence, clarity, and accuracy, ensuring all comparative claims are supported. Format the paper, compile the final reference list, and prepare for submission or dissemination.

## References

- An, Y., Yu, J. H., & James, S. (2025). **Investigating the higher education institutions’ guidelines and policies regarding the use of generative AI in teaching, learning, research, and administration.** *International Journal of Educational Technology in Higher Education, 22*(1), Article 10. https://doi.org/10.1186/s41239-025-00507-3

- Hung, J., & Chen, J. (2023). **The benefits, risks, and regulation of using ChatGPT in Chinese academia: A content analysis.** *Social Sciences, 12*(7), 380. https://doi.org/10.3390/socsci12070380

- Mo, H. (2025, February 27). **Chinese universities tighten AI usage rules to curb academic misuse.** *ECNS*.  ([Chinese universities tighten AI usage rules to curb academic misuse](http://www.ecns.cn/news/cns-wire/2025-02-27/detail-ihepcskv6108382.shtml#:~:text=In%20response%2C%20leading%20universities%2C%20including,what%20is%20permitted%20and%20prohibited)) ([Chinese universities tighten AI usage rules to curb academic misuse](http://www.ecns.cn/news/cns-wire/2025-02-27/detail-ihepcskv6108382.shtml#:~:text=Students%20are%20allowed%20to%20use,failing%20grades%20or%20degree%20revocation))

- *Stanford University Office of Community Standards.* (2023, February 16). **Generative AI policy guidance** (Honor Code implications of generative AI tools). Stanford University.  ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Absent%20a%20clear%20statement%20from,such%20assistance%20when%20in%20doubt)) ([Generative AI Policy Guidance | Office of Community Standards](https://communitystandards.stanford.edu/generative-ai-policy-guidance#:~:text=Individual%20course%20instructors%20are%20free,ask%20their%20instructors%20for%20clarification))

- Salamanca, S., Martinez, C. J., & Mezitis, T. A. (2023, September 22). **‘Struggling to keep up’: Harvard students and faculty grapple with impact of generative AI in classrooms.** *The Harvard Crimson*.  ([‘Struggling to Keep Up’: Harvard Students and Faculty Grapple with Impact of Generative AI in Classrooms | News | The Harvard Crimson](https://www.thecrimson.com/article/2023/9/22/generative-ai-policies-students-react/#:~:text=Though%20the%20Office%20of%20Undergraduate,generative%20AI%20in%20their%20courses)) ([‘Struggling to Keep Up’: Harvard Students and Faculty Grapple with Impact of Generative AI in Classrooms | News | The Harvard Crimson](https://www.thecrimson.com/article/2023/9/22/generative-ai-policies-students-react/#:~:text=Some%20instructors%20are%20taking%20a,stages%20of%20the%20writing%20process))

- *The Crimson Editorial Board.* (2023, February 3). **The (Mis)Education of ChatGPT.** *The Harvard Crimson*.  ([The (Mis)Education of ChatGPT | Opinion | The Harvard Crimson](https://www.thecrimson.com/article/2023/2/3/editorial-chatgpt-education/#:~:text=Speaking%20at%20a%20webinar%20at,on%20ChatGPT%20does%20not%20suffice)) ([The (Mis)Education of ChatGPT | Opinion | The Harvard Crimson](https://www.thecrimson.com/article/2023/2/3/editorial-chatgpt-education/#:~:text=the%20current%20lack%20of%20response,appears%20to%20be%20woefully%20unprepared))

- **Top university limits use of AI tools.** (2024, December 4). *China Daily*.  ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20university%20announced%20in%20its,writing%20process)) ([Top university limits use of AI tools - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202412/04/WS674fac9ba310f1265a1d0ece.html#:~:text=The%20stipulation%20clearly%20stated%20that,theses%20were%20completed%20using%20them))

